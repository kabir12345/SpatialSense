{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Torch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing\n",
    "from collections import Counter\n",
    "import torch\n",
    "import clip\n",
    "from fvcore.nn import flop_count\n",
    "from fvcore.nn.jit_handles import batchnorm_flop_jit, generic_activation_jit, get_shape\n",
    "\n",
    "# Helper function to compute the product of elements in a list\n",
    "def prod(lst):\n",
    "    result = 1\n",
    "    for x in lst:\n",
    "        result *= x\n",
    "    return result\n",
    "\n",
    "# Custom JIT handle definitions\n",
    "def generic_pooling_jit(name, multiplier=1):\n",
    "    def pool_jit(inputs: typing.List[object], outputs: typing.List[object]) -> typing.Counter[str]:\n",
    "        input_shape = get_shape(inputs[0])\n",
    "        output_shape = get_shape(outputs[0])\n",
    "        assert 2 <= len(input_shape) <= 5, input_shape\n",
    "        flop = prod(input_shape) + prod(output_shape)\n",
    "        flop_counter = Counter({name: flop * multiplier})\n",
    "        return flop_counter\n",
    "    return lambda inputs, outputs: pool_jit(inputs, outputs)\n",
    "\n",
    "def softmax_jit(inputs: typing.List[object], outputs: typing.List[object]) -> typing.Counter[str]:\n",
    "    input_shape = get_shape(inputs[0])\n",
    "    output_shape = get_shape(outputs[0])\n",
    "    flop = prod(input_shape) * 2 + prod(output_shape)\n",
    "    flop_counter = Counter({\"softmax\": flop})\n",
    "    return flop_counter\n",
    "\n",
    "def bmm_flop_jit(inputs: typing.List[object], outputs: typing.List[object]) -> typing.Counter[str]:\n",
    "    input1_shape = get_shape(inputs[0])\n",
    "    input2_shape = get_shape(inputs[1])\n",
    "    assert len(input1_shape) == len(input2_shape) == 3\n",
    "    assert input1_shape[0] == input2_shape[0] and input1_shape[2] == input2_shape[1], [input1_shape, input2_shape]\n",
    "    flop = prod(input1_shape) * input2_shape[-1]\n",
    "    flop_counter = Counter({\"bmm\": flop})\n",
    "    return flop_counter\n",
    "\n",
    "# Wrapper class for CLIP model\n",
    "class ForwardWrapper(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "    \n",
    "    def forward(self, *inputs):\n",
    "        return self.model(*inputs)\n",
    "\n",
    "# Load the CLIP model\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=\"cpu\")\n",
    "\n",
    "# Wrap the CLIP model for FLOP counting\n",
    "model_wrapper = ForwardWrapper(model.visual)\n",
    "\n",
    "# Prepare a sample input (e.g., a batch with a single image)\n",
    "image = torch.randn(1, 3, 224, 224)  # Example input tensor\n",
    "\n",
    "# Define supported operations for FLOP counting\n",
    "supported_ops = {\n",
    "    \"aten::batch_norm\": batchnorm_flop_jit,\n",
    "    \"aten::relu\": generic_activation_jit(\"relu\"),\n",
    "    \"aten::linear\": bmm_flop_jit,\n",
    "    \"aten::adaptive_avg_pool2d\": generic_pooling_jit(\"adaptive_avg_pool2d\"),\n",
    "    \"aten::softmax\": softmax_jit,\n",
    "    # Add more operations as needed based on the CLIP model architecture\n",
    "}\n",
    "\n",
    "# Perform FLOP counting\n",
    "flops, _ = flop_count(model_wrapper, inputs=(image,), supported_ops=supported_ops)\n",
    "\n",
    "print(f\"Total FLOPs for a single inference pass: {flops}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
